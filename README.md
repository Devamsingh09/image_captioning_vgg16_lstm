
# üñºÔ∏èüîä Image Caption Generator with Audio (VGG16 + LSTM)

An end-to-end **Image Captioning application** that generates descriptive captions for images and converts the caption into **audio** using Google Text-to-Speech.  
The project is deployed using **Streamlit** and demonstrates the complete deep learning workflow ‚Äî from preprocessing to deployment.

---

## üöÄ Demo Features

- üì∏ Upload an image (jpg / png / jpeg)
- üß† Generate an English caption using a trained CNN‚ÄìLSTM model
- üîä Convert the generated caption into speech (audio output)
- ‚ö° Fast inference with locally saved models (no re-downloads)

---

## üß† Model Architecture

### üîπ Image Encoder
- **VGG16** (pretrained on ImageNet)
- Uses the **penultimate fully connected layer** as image features

### üîπ Caption Decoder
- **LSTM-based language model**
- Generates captions word-by-word using greedy decoding

### üîπ Training Dataset
- Flickr-style image caption dataset  
- Start / end tokens used for sequence modeling

---

## üìà Evaluation

The model was evaluated using BLEU metrics:

| Metric | Score |
|------|------|
| BLEU-1 | **0.52** |
| BLEU-2 | **0.30** |

‚úÖ These scores indicate good object recognition and reasonable sentence fluency for a CNN‚ÄìLSTM baseline model.

---

## üõ†Ô∏è Tech Stack

- **Python 3.11**
- **TensorFlow / Keras**
- **Streamlit**
- **VGG16 (CNN)**
- **LSTM**
- **Google Text-to-Speech (gTTS)**
- **Git & Git LFS**

---

## üìÅ Project Structure

```

image_captioning/
‚îÇ
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îî‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ model.keras                     # Trained captioning model
‚îú‚îÄ‚îÄ tokenizer.pkl                  # Tokenizer
‚îú‚îÄ‚îÄ features.pkl                   # Extracted image features
‚îî‚îÄ‚îÄ vgg16_feature_extractor.keras  # Locally saved VGG16

````

> ‚ö†Ô∏è Large model files are tracked using **Git LFS**.

---

## ‚ñ∂Ô∏è How to Run Locally

### 1Ô∏è‚É£ Clone the repository
```bash
git clone <repo-url>
cd image_captioning
````

### 2Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Run the Streamlit app

```bash
python -m streamlit run app.py
```

The app will open automatically in your browser.

---

## üîä Caption + Audio Generation Flow

```
Image Upload
     ‚Üì
VGG16 Feature Extraction
     ‚Üì
LSTM Caption Generation
     ‚Üì
Text Caption
     ‚Üì
Google Text-to-Speech
     ‚Üì
Audio Output
```

---

## ‚ö†Ô∏è Known Limitations

* Uses **greedy decoding** (can cause repetitive captions)
* No attention mechanism
* CNN features are global (no object-level focus)

These are known limitations of CNN‚ÄìLSTM architectures.

---

## üå± Future Improvements

* ‚úÖ Beam Search decoding
* ‚úÖ Attention mechanism
* ‚úÖ Transformer-based models (BLIP / ViT)
* ‚úÖ Multilingual captioning
* ‚úÖ Deployment to Streamlit Cloud / HuggingFace Spaces

---

## üéØ Learning Outcomes

* Built an end-to-end image captioning pipeline
* Understood sequence modeling with LSTMs
* Debugged real deployment issues (RGBA images, pickle errors, model loading)
* Deployed a multimodal AI app with audio output
* Used **Git LFS** for large deep learning models

---

## üë§ Author

**Devam Singh**
B.Tech CSE (DSAI), Class of 2026

üìß Email: [devamsingh0009@gmail.com](mailto:devamsingh0009@gmail.com)
üîó GitHub: [https://github.com/Devamsingh09](https://github.com/Devamsingh09)
üîó LinkedIn: [https://linkedin.com/in/devam-singh-248025265/](https://linkedin.com/in/devam-singh-248025265/)

---

## ‚≠ê Acknowledgements

* TensorFlow & Keras
* Streamlit
* Google Text-to-Speech
* Flickr Image Caption Dataset

---

> ‚≠ê If you find this project useful, feel free to star the repository!

```



Just tell me üëç
```
